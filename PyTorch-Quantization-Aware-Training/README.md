
## PyTorch-Quantization-Aware-Training


## Model Result
- FP32 evaluation accuracy: 0.869
- INT8 evaluation accuracy: 0.867
- FP32 CPU Inference Latency: 4.36 ms / sample
- FP32 CUDA Inference Latency: 3.55 ms / sample
- INT8 CPU Inference Latency: 1.85 ms / sample
- INT8 JIT CPU Inference Latency: 0.41 ms / sample
